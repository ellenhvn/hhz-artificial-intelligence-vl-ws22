{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Wisconsin Data Set\n",
    "\n",
    "Create a predictive model that classifies benign vs. malignant tumors. \n",
    "See https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/data for data understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(\"data.csv\")\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.describe(include=\"all\")  # descriptive statistics for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.isnull().sum()  # check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data[original_data.duplicated(keep=False)]  # check for duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values and no duplicates, so you don't have to take actions here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data[[\"radius_mean\", \"diagnosis\"]].groupby(\n",
    "    [\"diagnosis\"], as_index=False\n",
    ").mean().sort_values(by=\"diagnosis\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect more feature, e.g. texture, perimeter,... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step during feature selection is removing features that strongly correlate with each other. You keep only one feature as \"representer\" of the information and remove redundant features. There are more advanced methods to do this but, for now, just look at the correlation map and decide which features to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(original_data.corr(), annot=True, linewidths=0.5, fmt=\".1f\", ax=ax)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title(\"Correlation Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced_features = original_data[[\"<your feature 1>\", \"<your feature 2>\", \"...\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, have a look at the correlation map and remove more features if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(data_reduced_features.corr(), annot=True, linewidths=0.5, fmt=\".1f\", ax=ax)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title(\"Correlation Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set X and y (predictors and target) according to your dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_reduced_features['<your target column>']\n",
    "predictors = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    predictors, target, test_size=0.2, random_state=123\n",
    ")  # 80-20 split into training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check if the dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use StandardScaler to scale your predictors (fit on training set and transform training and test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = # your code \n",
    "# your code\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification models and evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a decision tree classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# your code\n",
    "\n",
    "print(\"train performance\")\n",
    "print(classification_report(y_train, tree.predict(X_train)))\n",
    "print(\"test performance\")\n",
    "print(classification_report(y_test, tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you evaluate this result (hints: overfitting vs. underfitting, which metric might be important for the use case and why)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, tree.predict(X_test))\n",
    "df_cm = pd.DataFrame(\n",
    "    conf_mat,\n",
    "    index=[\"B\", \"M\"],\n",
    "    columns=[\"B\", \"M\"],\n",
    ")\n",
    "fig = plt.figure(figsize=[10, 7])\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "heatmap.yaxis.set_ticklabels(\n",
    "    heatmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\", fontsize=14\n",
    ")\n",
    "heatmap.xaxis.set_ticklabels(\n",
    "    heatmap.xaxis.get_ticklabels(), rotation=45, ha=\"right\", fontsize=14\n",
    ")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a logistic regression model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# your code\n",
    "print(\"train performance\")\n",
    "print(classification_report(y_train, logreg.predict(X_train)))\n",
    "print(\"test performance\")\n",
    "print(classification_report(y_test, logreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you evaluate this result? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, logreg.predict(X_test))\n",
    "df_cm = pd.DataFrame(\n",
    "    conf_mat,\n",
    "    index=[\"B\", \"M\"],\n",
    "    columns=[\"B\", \"M\"],\n",
    ")\n",
    "fig = plt.figure(figsize=[10, 7])\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "heatmap.yaxis.set_ticklabels(\n",
    "    heatmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\", fontsize=14\n",
    ")\n",
    "heatmap.xaxis.set_ticklabels(\n",
    "    heatmap.xaxis.get_ticklabels(), rotation=45, ha=\"right\", fontsize=14\n",
    ")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feel free to try out more classifiers (don't forget to import required packages!), change classifier parameters, modify train and test split, select other predictors,..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
