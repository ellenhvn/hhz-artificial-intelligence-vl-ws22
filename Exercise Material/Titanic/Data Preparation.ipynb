{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and writing files is slightly different between Local and Cloud. In the Cloud, the easiest way is to use project-lib (see https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html?audience=wdp if you want to learn more). \n",
    "When opening this notebook in Watson Studio Cloud for the first time, insert a project token by clicking on the \"hamburger\" icon on the right hand side. If no project token exists, follow the link to create a new one (Manage --> Access control --> Access token --> New token, with Editor role). Then, return to this notebook and repeat the steps to insert a project token. This will now add an additional cell above. Run this cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRISP-DM is the \"CRoss Industry Standard Process for Data Mining\" which is one of the frequently used guidelines in data analytics. \n",
    "\n",
    "This notebook regards the phases \"Data Understanding\" and \"Data Preparation\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "display(Image('https://www.kdnuggets.com/wp-content/uploads/crisp-dm-4-problems-fig1.png', width=500, unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle Titanic data set is often used to do first steps in data science / machine learning. It contains certain personal information about passengers. The task is to prepare raw data and create prediction models, so-called classifiers, that predict if a person survived or not. Data are available here: https://www.kaggle.com/c/titanic/data - refer to this page also for further information, e.g. a data dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: As stated previously, loading data differs between local and cloud versions, select the right one depending on the platform used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local: Fetch the file\n",
    "# original_data = pd.read_csv('train.csv') # Local, use full path if notebook and file in different folders! \n",
    "\n",
    "# Cloud: Fetch the file\n",
    "my_file = project.get_file(\"train.csv\")\n",
    "\n",
    "# Cloud: Read the CSV data file from the object storage into a pandas DataFrame\n",
    "my_file.seek(0)\n",
    "original_data = pd.read_csv(my_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.describe(include='all') # descriptive statistics for all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data[original_data.duplicated(keep=False)] # check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data['PassengerId'][original_data['PassengerId'].duplicated(keep=False)] # check for duplicate PassengerIds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no obvious duplicates in the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values and adjust data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three columns that contain missing values. Simple ways to handle this data quality issues are \n",
    "\n",
    "a) drop the rows where a specific value is missing (here done for 'Age'), \n",
    "\n",
    "b) drop the whole column if too many values are missing (here: 'Cabin'), \n",
    "\n",
    "c) replace missing values with the most frequent value (here: 'Embarked'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null = original_data.dropna(axis=0, subset=['Age']) # drop rows where 'Age' is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null = df_wo_null.drop(['Cabin'], axis = 1) # drop column 'Cabin' since there are too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null['Embarked'] = df_wo_null['Embarked'].fillna(df_wo_null['Embarked'].mode().iloc[0]) # replace missing 'Embarked' with the most frequent value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find predictors and edit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, predictors are defined. These are columns (from now on called features) that are assumed to have an influence on the target, in this case 'Survived: yes or no?'. Besides, new features are derived from existing ones. This process is called 'feature engineering' and is one of the key steps during data preparation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null = df_wo_null.drop(['Ticket','PassengerId'], axis = 1) # IDs are no suitable predictors\n",
    "df_wo_null = df_wo_null.drop(['Name'], axis = 1) # 'Name' is no appropriate predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival rate per feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
    "# survival rate per class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = sns.FacetGrid(df_wo_null, col='Survived')\n",
    "viz.map(plt.hist, 'Age', bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null['AgeRange'] = pd.cut(df_wo_null['Age'], bins=8) # summarize 'Age' in ranges\n",
    "df_wo_null[['AgeRange', 'Survived']].groupby(['AgeRange'], as_index=False).mean().sort_values(by='AgeRange', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null['AgeRange'] = pd.cut(df_wo_null['Age'], bins=8, labels = ['0-10','10-20','20-30','30-40','40-50','50-60','60-70', '70-80'])\n",
    "# create a new column 'AgeRange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null = df_wo_null.drop(['Age'], axis=1) # remove 'Age' since information is now contained in 'AgeRange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = sns.distplot(df_wo_null['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null['FareRange'] = pd.qcut(df_wo_null['Fare'], q=4) # create quartiles from fare price \n",
    "df_wo_null[['FareRange', 'Survived']].groupby(['FareRange'], as_index=False).mean().sort_values(by='FareRange', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null['FareRange'] = pd.qcut(df_wo_null['Fare'], q=4, labels = ['Q1', 'Q2', 'Q3', 'Q4']) # add 'FareRange' as a new column\n",
    "df_wo_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null = df_wo_null.drop(['Fare'], axis = 1) # remove 'Fare' since information is now contained in 'FareRange' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null['Family'] = df_wo_null['SibSp'] + df_wo_null['Parch'] + 1 # calculate family size and add this as a new column\n",
    "\n",
    "df_wo_null[['Family', 'Survived']].groupby(['Family'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null = df_wo_null.drop(['SibSp', 'Parch'], axis = 1) # remove columns since information is now contained in 'Family'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to try out additional idea, e.g. extract title from name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_null.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df_wo_null, drop_first=True) # 0-1 encoding for categorical values\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: Select the correct way to export csv file here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "# df_dummies.to_csv('train_dummies.csv', index = False) # full path if file should not be in the same folder as the notebook\n",
    "\n",
    "# Cloud\n",
    "project.save_data(\"train_dummies.csv\", df_dummies.to_csv(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
